= Install Advanced Custer Management and GitOps Operators
include::_attributes.adoc[]

This section will guide you through the ACM and GitOps CD Operators installation and the ACM/GitOps integration in order to deploy application both local and remote clusters. 

Hereâ€™s a list of required operators to install through the Operator Lifecycle Manager (OLM), which manages the installation, upgrade, and removal:

 - **Advanced Custer Management Operator**
 
image::deploy/deploy01.png[]
 
 - **GitOps Operator**
 
image::deploy/deploy02.png[]

[NOTE]
====
- The OpenShift Container Platform provided in this lab meets the minimal https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/install/installing#requirements-and-recommendations[requirements] from the official documentation. The cluster hub components will be installed on worker nodes, no https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/install/installing#installing-on-infra-node[infrastructure] nodes will be provided.
- OpenShift Container Platform required access: Cluster administrator (cluster-admin) permissions.
====

[#install]
== Install Advanced Custer Management Operator

We will go through the Advanced Custer Management Operator installation following below steps:

- **Enable master node scheduling**

Let's start getting our hands dirty. We are going to enable master nodes for scheduling workloads to take advantage of all limited resources that we have in this lab.

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm replace -k 02_bootstrap/base/openshift/base
----

Let's check the results as follows:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm get scheduler cluster -o yaml -o=jsonpath='{.spec.mastersSchedulable}{"\n"}'
----

We are expecting **true** as a  result.

WARNING: This configuration is not recommended for production.

TIP: https://access.redhat.com/articles/2988581[OpenShift Admins Guide to jsonpath]

- **Install Advanced Custer Management Operator**:

Let's install the Advanced Cluster Management Operator running the following command which will create the following resources:

* `Namespace` - open-cluster-management
* `OperatorGroup` - open-cluster-management-rhte2023
* `Subscription` - Advanced Cluster Management Operator

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm apply -k 02_bootstrap/base/acm_operator/base
----

Let's check the results as follows:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
watch oc --context acm get pod -n open-cluster-management
----

Wait until all pods are in `Running` status and `Ready`. It can take a couple of minutes approx.

After installing the Advanced Cluster Management Operator, we also need to create the CRD object `MultiClusterHub`. 

- **Create `MultiClusterHub`**:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm apply -k 02_bootstrap/base/acm_multiclusterhub/base
----

Let's check the results as follows:

[.lines_space]
[.console-input]
[source,shell, subs="+macros,+attributes"]
----
watch oc --context acm get mch -o=jsonpath='{.items[0].status.phase}' -n open-cluster-management
----

Wait until the Advanced Cluster Management Operatator is `Runnig` status, in the meanwhile the status is `Installing`. It can take up to 10 minutes.

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
watch oc --context acm get pod -n open-cluster-management
----

If the multiclusterhub is not in `Running` status after a while, check the operator logs or pods status.

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm logs multiclusterhub-operator-xxx-xxx -n open-cluster-management
----

[#console]
== Access to the Advanced Cluster Management Web Console

Advanced Cluster Management Console is integrated with the Openshift Console, so once the ACM Operator is installed, the Openshift Console will show a new item "All Clusters" on the side navigation menu as below:

Console: https://console-openshift-console.apps.<your_domain>/
User: kubeadmin
Password: <your_passoword>

image::deploy/deploy00.png[]

**All Cluster** will provide access to the Advanced Cluster Management Console and "local-cluster" will provide access to the Openshift Cluster.

Also, the ACM provides a dashboard UI. We can access it through a OpenShift `Route`. List routes in the `open-cluster-management` namespace and get the address of console https://multicloud-console.apps.<YOUR_DOMAIN>. But, It will be deprecated in ACM 2.7

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm get route -n open-cluster-management
----

Let's switch to the Advanced Cluster Management Console and Login with Openshift to check that it is working properly.

image::deploy/deploy21.png[]

[#gitops]
== Install GitOps Operator

The next step is the https://docs.openshift.com/container-platform/4.10/cicd/gitops/installing-openshift-gitops.html[OpenShift GitOps Operator] installation. OpenShift GitOps uses https://argo-cd.readthedocs.io/en/stable/[Argo CD] to manage specific cluster-scoped resources. Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes.

Let's install it:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm apply -k 02_bootstrap/base/gitops_operator/base/
----

Let's check the results as follows:

- Wait until pods in `openshift-gitops` namespace in `Running` status and `Ready`. It can takes a up 2 minutes.

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
watch oc --context acm get pods -n openshift-gitops
----

- Tracking Kubernetes resources by label

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc patch argocd openshift-gitops -n openshift-gitops --type='json' -p='[{"op": "add", "path": "/spec/resourceTrackingMethod", "value": "annotation" }]'
----

- ArgoCD dashboard UI is accessible from the Openshift Console

image::deploy/deploy13.png[]

image::deploy/deploy14.png[]

Also, an Openshift `Route` is created:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm get route -n openshift-gitops
----

- Login into the ArgoCD Console with the Openshift credentials. 

image::deploy/deploy15.png[]

[#gitopsacm]
== Integrate Openshift GitOps and Advanced Cluster Management

Once we have completed the Advanced Cluster Management and Openshift Gitops Operators installation, we will go through the integration process between them. So, this process will let us:

- Deploy and discover ArgoCD multicluster applications
- Setup Git or Helm Applications based repositories
- Configure sync policies for our applications
- Configure placement based on labels or clustersets

Advanced Cluster Management introduces a new `gitopscluster` resource kind, which connects to a `placement` resource to determine which clusters to import into Argo CD. This integration allows you to expand your fleet, while having Argo CD automatically engage in working with your new clusters. This means if you leverage Argo CD `ApplicationSets`, your application payloads are automatically applied to your new clusters as they are registered by Advanced Cluster Management in your Argo CD instances.

image::deploy/deploy18.png[]

Let's run the following commands to perform the Openshift GitOps Operator integration with Advanced Cluster Management

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm apply -k 02_bootstrap/base/gitops_acm/base/
----

It will create:

* a `ManagedClusterSet` - **rhte2023-gitops-clusters**. `ManagedClusterSet` resources allow the grouping of cluster resources. We will add both Openshift Cluster `local-cluster` and `rhte2023-cluster01` into this `ManagedClusterSet` which enables deployment across all of the Openshift Cluster.

* a `ManagedClusterSetBinding` - **rhte2023-gitops-clusters**. `ManagedClusterSetBinding` resource will bind a `ManagedClusterSet` **rhte2023-gitops-clusters** to a `namespace` **openshift-gitops**. It means that applications and policies that are created in the `openshift-gitops` namespace can only access managed clusters that are included in the bound managed cluster set resource.

* a `Placement` - **rhte2023-gitops-clusters-placement** based on a ManagedCluster label **vendor=Openshift** and claim **platform=AWS**

[.lines_space]
[.console-input]
[source,yaml, subs="+macros,+attributes"]
----
---
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: rhte2023-gitops-clusters-placement
  namespace: openshift-gitops
spec:
  predicates:
    - requiredClusterSelector:
        labelSelector:
          matchLabels:
            vendor: OpenShift
        claimSelector:
          matchExpressions:
            - key: platform.open-cluster-management.io
              operator: In
              values:
                - AWS
----

We will see more `Placements` examples on the next sections.

TIP: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/multicluster_engine/index#placement-binding[Placement Examples] to select a cluster wih the largest allocatable memory and/or CPU.

NOTE: When you integrate ACM with the GitOps operator for every managed cluster that is bound to the GitOps namespace through the placement and ManagedClusterSetBinding custom resources, a secret with a token to access the ManagedCluster is created in the namespace. This is required for the GitOps controller to sync resources to the managed cluster. When a user is given administrator access to a GitOps namespace to perform application lifecycle operations, the user also gains access to this secret and admin level to the managed cluster

[#managedcluster]
== Registering local-cluster to GitOps

After the ACM/GitOps integration is done, let's add `local-cluster` to `ClusterSet` **rhte2023-gitops-clusters**. This configuration will let us deploy applications matching Clusters **vendor=Openshift** and **platform=AWS**.

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm label ManagedCluster local-cluster cluster.open-cluster-management.io/clusterset=rhte2023-gitops-clusters --overwrite
----

and check that cluster `local-cluster` is added as an ArgoCD Cluster from the **ArgoCD Console > Settings > Clusters** where we expect to see `local-cluster` as one of the clusters

image::deploy/deploy16.png[]

image::deploy/deploy17.png[]

and the `local-cluster` belongs to `ClusterSet rhte2023-gitops-clusters` through `Openshift Console > Advanced Cluster Management > Clusters > Cluster sets`

image::deploy/deploy20.png[]

[#deployall]
== Argo CD â€” App Of Everything

Weel done, keep pushing! 

This is the last step of this section where will automate with ArgoCD everything that we were doing so far. Let's change the `ApplicationSet` YAML file according to your environment. 
 
[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
vi rhte2023-bootstrap-gitops.yaml
----

and change `server` to you `` and `repoURL` to your git repository forked:

[.lines_space]
[.console-input]
[source,yaml, subs="+macros,+attributes"]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: rhte2023-bootstrap-gitops
  namespace: openshift-gitops
spec:
  destination:
    namespace: openshift-gitops
    server: https://<your_cluster>:6443
  project: default
  source:
    path: rhte2023/02_bootstrap/base
    repoURL: https://github.com/<your_github_account>/rhte-2023-acm-apps.git
----

finally, let's create the ApplicationSet:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc --context acm apply -f 02_bootstrap/argocd/rhte2023-bootstrap-gitops.yaml
----

Let's check the results as follows:

ACM > Applications > Search > bootstrap

image::deploy/deploy20.png[]

ArgoCD Console > Applications 

image::deploy/deploy23.png[]

